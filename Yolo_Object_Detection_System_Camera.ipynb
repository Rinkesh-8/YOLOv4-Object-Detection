{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d842fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2c2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model \n",
    "yolo_model = cv2.dnn.readNetFromDarknet(\"S:/Python-Anaconda/Anaconda/pkgs/darknet-yolov4-0.0.1-hd8ed1ab_0/models/darknet/yolov4/yolov4.config\",\n",
    "                                        \"S:/Python-Anaconda/Anaconda/pkgs/darknet-yolov4-0.0.1-hd8ed1ab_0/models/darknet/yolov4/yolov4.weights\")\n",
    "\n",
    "model_layers = yolo_model.getLayerNames()\n",
    "output_layers = [model_layers[model_layer - 1] for model_layer in yolo_model.getUnconnectedOutLayers()]\n",
    "\n",
    "# define class labels\n",
    "class_labels_path = \"S:/Python-Anaconda/Anaconda/pkgs/darknet-yolov4-0.0.1-hd8ed1ab_0/models/darknet/yolov4/yolov4.labels\"\n",
    "class_labels = open(class_labels_path).read().strip().split(\"\\n\")\n",
    "\n",
    "# declare repeating bounding box colors for each class \n",
    "class_colors = [\"255,0,0\",\"0,255,0\",\"0,0,255\",\"255,155,0\",\"255,0,255\"]\n",
    "class_colors = [np.array(every_color.split(\",\")).astype(\"int\") for every_color in class_colors]\n",
    "class_colors = np.array(class_colors)\n",
    "class_colors = np.tile(class_colors, (16, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e6e19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection_analysis(frame, obj_detections_in_layers, confidence_threshold): \n",
    " \n",
    "  # get the image dimensions  \n",
    "  img_height = frame.shape[0]\n",
    "  img_width = frame.shape[1]\n",
    " \n",
    "  result = frame.copy()\n",
    "  \n",
    "  # loop over each output layer \n",
    "  for object_detections_in_single_layer in obj_detections_in_layers:\n",
    "    # loop over the detections in each layer\n",
    "      for object_detection in object_detections_in_single_layer:  \n",
    "        # obj_detection[1]: bbox center pt_x\n",
    "        # obj_detection[2]: bbox center pt_y\n",
    "        # obj_detection[3]: bbox width\n",
    "        # obj_detection[4]: bbox height\n",
    "        # obj_detection[5]: confidence scores for all detections within the bbox \n",
    " \n",
    "        # get the confidence scores of all objects detected with the bounding box\n",
    "        prediction_scores = object_detection[5:]\n",
    "        \n",
    "        predicted_class_id = np.argmax(prediction_scores)\n",
    "        # get the prediction confidence\n",
    "        prediction_confidence = prediction_scores[predicted_class_id]\n",
    "    \n",
    "        # consider object detections with confidence score higher than threshold\n",
    "        if prediction_confidence > confidence_threshold:\n",
    "            # get the predicted label\n",
    "            predicted_class_label = class_labels[predicted_class_id]\n",
    "            # compute the bounding box coordinates scaled for the input image \n",
    "            \n",
    "            bounding_box = object_detection[0:4] * np.array([img_width, img_height, img_width, img_height])\n",
    "            # get the bounding box centroid (x,y), width and height as integers\n",
    "            (box_center_x_pt, box_center_y_pt, box_width, box_height) = bounding_box.astype(\"int\")\n",
    "            \n",
    "            start_x_pt = max(0, int(box_center_x_pt - (box_width / 2)))\n",
    "            start_y_pt = max(0, int(box_center_y_pt - (box_height / 2)))\n",
    "            end_x_pt = start_x_pt + box_width\n",
    "            end_y_pt = start_y_pt + box_height\n",
    "            \n",
    "            # get a random mask color from the numpy array of colors\n",
    "            box_color = class_colors[predicted_class_id]\n",
    "            \n",
    "            # convert the color numpy array as a list and apply to text and box\n",
    "            box_color = [int(c) for c in box_color]\n",
    "            \n",
    "            # print the prediction in console\n",
    "            predicted_class_label = \"{}: {:.2f}%\".format(predicted_class_label, prediction_confidence * 100)\n",
    "            print(\"predicted object {}\".format(predicted_class_label))\n",
    "            \n",
    "            # draw the rectangle and text in the image\n",
    "            cv2.rectangle(result, (start_x_pt, start_y_pt), (end_x_pt, end_y_pt), box_color, 1)\n",
    "            cv2.putText(result, predicted_class_label, (start_x_pt, start_y_pt-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 1)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae20401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted object backpack: 40.70%\n",
      "predicted object backpack: 42.35%\n",
      "predicted object backpack: 21.93%\n",
      "predicted object backpack: 26.02%\n",
      "predicted object backpack: 45.37%\n",
      "predicted object backpack: 45.88%\n",
      "predicted object backpack: 25.82%\n",
      "predicted object backpack: 29.04%\n",
      "predicted object chair: 31.62%\n",
      "predicted object chair: 75.25%\n",
      "predicted object chair: 80.20%\n",
      "predicted object backpack: 20.22%\n",
      "predicted object backpack: 20.96%\n",
      "predicted object chair: 21.30%\n",
      "predicted object chair: 30.26%\n",
      "predicted object person: 62.19%\n",
      "predicted object person: 35.38%\n",
      "predicted object person: 31.62%\n",
      "predicted object backpack: 70.21%\n",
      "predicted object backpack: 69.36%\n",
      "predicted object backpack: 44.72%\n",
      "predicted object backpack: 44.77%\n",
      "predicted object chair: 37.25%\n",
      "predicted object chair: 37.83%\n",
      "predicted object chair: 24.63%\n",
      "predicted object chair: 77.81%\n",
      "predicted object chair: 79.23%\n",
      "predicted object chair: 51.91%\n",
      "predicted object backpack: 31.09%\n",
      "predicted object backpack: 31.91%\n",
      "predicted object chair: 26.64%\n",
      "predicted object chair: 27.58%\n",
      "predicted object person: 41.15%\n",
      "predicted object person: 44.43%\n",
      "predicted object person: 88.21%\n",
      "predicted object person: 88.55%\n",
      "predicted object person: 21.74%\n",
      "predicted object backpack: 40.32%\n",
      "predicted object backpack: 41.88%\n",
      "predicted object backpack: 20.08%\n",
      "predicted object backpack: 24.11%\n",
      "predicted object backpack: 49.06%\n",
      "predicted object backpack: 49.34%\n",
      "predicted object backpack: 26.83%\n",
      "predicted object backpack: 30.00%\n",
      "predicted object chair: 32.39%\n",
      "predicted object chair: 76.91%\n",
      "predicted object chair: 81.29%\n",
      "predicted object chair: 24.63%\n",
      "predicted object chair: 33.95%\n",
      "predicted object person: 75.51%\n",
      "predicted object person: 59.17%\n",
      "predicted object backpack: 73.68%\n",
      "predicted object backpack: 72.61%\n",
      "predicted object backpack: 42.56%\n",
      "predicted object backpack: 42.50%\n",
      "predicted object chair: 38.24%\n",
      "predicted object chair: 38.65%\n",
      "predicted object chair: 23.23%\n",
      "predicted object chair: 79.84%\n",
      "predicted object chair: 81.22%\n",
      "predicted object chair: 51.81%\n",
      "predicted object backpack: 29.72%\n",
      "predicted object backpack: 30.44%\n",
      "predicted object chair: 31.53%\n",
      "predicted object chair: 32.40%\n",
      "predicted object person: 92.54%\n",
      "predicted object person: 92.49%\n",
      "predicted object person: 21.23%\n",
      "predicted object person: 24.57%\n",
      "predicted object backpack: 40.29%\n",
      "predicted object backpack: 41.87%\n",
      "predicted object backpack: 21.91%\n",
      "predicted object backpack: 26.09%\n",
      "predicted object backpack: 49.26%\n",
      "predicted object backpack: 49.74%\n",
      "predicted object backpack: 27.21%\n",
      "predicted object backpack: 30.43%\n",
      "predicted object chair: 31.07%\n",
      "predicted object chair: 73.06%\n",
      "predicted object chair: 78.34%\n",
      "predicted object chair: 21.23%\n",
      "predicted object chair: 30.24%\n",
      "predicted object person: 76.17%\n",
      "predicted object person: 56.78%\n",
      "predicted object backpack: 71.61%\n",
      "predicted object backpack: 70.68%\n",
      "predicted object backpack: 45.47%\n",
      "predicted object backpack: 45.47%\n",
      "predicted object chair: 36.71%\n",
      "predicted object chair: 37.16%\n",
      "predicted object chair: 23.13%\n",
      "predicted object chair: 77.70%\n",
      "predicted object chair: 79.03%\n",
      "predicted object chair: 51.76%\n",
      "predicted object backpack: 29.16%\n",
      "predicted object backpack: 29.89%\n",
      "predicted object chair: 27.23%\n",
      "predicted object chair: 28.25%\n",
      "predicted object person: 21.57%\n",
      "predicted object person: 92.00%\n",
      "predicted object person: 91.94%\n",
      "predicted object person: 22.84%\n",
      "predicted object backpack: 33.70%\n",
      "predicted object backpack: 35.12%\n",
      "predicted object backpack: 23.73%\n",
      "predicted object backpack: 41.74%\n",
      "predicted object backpack: 42.20%\n",
      "predicted object backpack: 25.78%\n",
      "predicted object backpack: 28.79%\n",
      "predicted object chair: 33.11%\n",
      "predicted object chair: 74.96%\n",
      "predicted object chair: 80.00%\n",
      "predicted object backpack: 21.40%\n",
      "predicted object backpack: 22.42%\n",
      "predicted object chair: 23.36%\n",
      "predicted object chair: 33.20%\n",
      "predicted object person: 75.64%\n",
      "predicted object person: 54.56%\n",
      "predicted object backpack: 64.72%\n",
      "predicted object backpack: 63.93%\n",
      "predicted object backpack: 43.91%\n",
      "predicted object backpack: 43.77%\n",
      "predicted object chair: 38.30%\n",
      "predicted object chair: 38.73%\n",
      "predicted object chair: 25.37%\n",
      "predicted object chair: 78.66%\n",
      "predicted object chair: 80.07%\n",
      "predicted object chair: 51.53%\n",
      "predicted object backpack: 32.95%\n",
      "predicted object backpack: 33.77%\n",
      "predicted object chair: 29.65%\n",
      "predicted object chair: 30.63%\n",
      "predicted object person: 91.26%\n",
      "predicted object person: 91.18%\n",
      "predicted object person: 20.14%\n",
      "predicted object person: 23.48%\n",
      "predicted object backpack: 44.00%\n",
      "predicted object backpack: 45.70%\n",
      "predicted object backpack: 21.09%\n",
      "predicted object backpack: 25.19%\n",
      "predicted object backpack: 54.25%\n",
      "predicted object backpack: 54.47%\n",
      "predicted object backpack: 26.67%\n",
      "predicted object backpack: 29.70%\n",
      "predicted object chair: 32.10%\n",
      "predicted object chair: 74.33%\n",
      "predicted object chair: 79.24%\n",
      "predicted object backpack: 20.04%\n",
      "predicted object chair: 23.03%\n",
      "predicted object chair: 32.25%\n",
      "predicted object person: 70.52%\n",
      "predicted object person: 70.55%\n",
      "predicted object backpack: 75.18%\n",
      "predicted object backpack: 74.12%\n",
      "predicted object backpack: 44.30%\n",
      "predicted object backpack: 44.54%\n",
      "predicted object chair: 38.61%\n",
      "predicted object chair: 39.20%\n",
      "predicted object chair: 25.17%\n",
      "predicted object chair: 76.82%\n",
      "predicted object chair: 78.35%\n",
      "predicted object chair: 49.90%\n",
      "predicted object backpack: 29.43%\n",
      "predicted object backpack: 30.16%\n",
      "predicted object chair: 28.28%\n",
      "predicted object chair: 29.43%\n",
      "predicted object person: 91.80%\n",
      "predicted object person: 91.82%\n",
      "predicted object person: 24.85%\n",
      "predicted object person: 28.18%\n",
      "predicted object backpack: 44.28%\n",
      "predicted object backpack: 45.81%\n",
      "predicted object backpack: 20.88%\n",
      "predicted object backpack: 24.93%\n",
      "predicted object backpack: 53.48%\n",
      "predicted object backpack: 53.51%\n",
      "predicted object backpack: 26.40%\n",
      "predicted object backpack: 29.35%\n",
      "predicted object chair: 30.25%\n",
      "predicted object chair: 67.94%\n",
      "predicted object chair: 74.42%\n",
      "predicted object chair: 25.53%\n",
      "predicted object person: 77.08%\n",
      "predicted object person: 59.11%\n",
      "predicted object backpack: 75.92%\n",
      "predicted object backpack: 74.81%\n",
      "predicted object backpack: 42.80%\n",
      "predicted object backpack: 43.05%\n",
      "predicted object chair: 34.08%\n",
      "predicted object chair: 34.75%\n",
      "predicted object chair: 22.04%\n",
      "predicted object chair: 72.20%\n",
      "predicted object chair: 73.66%\n",
      "predicted object chair: 46.75%\n",
      "predicted object backpack: 29.62%\n",
      "predicted object backpack: 30.34%\n",
      "predicted object chair: 22.60%\n",
      "predicted object chair: 23.56%\n",
      "predicted object person: 92.37%\n",
      "predicted object person: 92.41%\n",
      "predicted object person: 20.83%\n",
      "predicted object person: 23.96%\n",
      "predicted object backpack: 40.23%\n",
      "predicted object backpack: 41.68%\n",
      "predicted object backpack: 20.59%\n",
      "predicted object backpack: 24.80%\n",
      "predicted object backpack: 48.10%\n",
      "predicted object backpack: 48.34%\n",
      "predicted object backpack: 25.74%\n",
      "predicted object backpack: 28.69%\n",
      "predicted object chair: 28.66%\n",
      "predicted object chair: 75.78%\n",
      "predicted object chair: 80.36%\n",
      "predicted object backpack: 20.06%\n",
      "predicted object chair: 24.50%\n",
      "predicted object chair: 34.01%\n",
      "predicted object person: 76.88%\n",
      "predicted object person: 62.15%\n",
      "predicted object backpack: 70.70%\n",
      "predicted object backpack: 69.70%\n",
      "predicted object backpack: 42.83%\n",
      "predicted object backpack: 42.65%\n",
      "predicted object chair: 33.60%\n",
      "predicted object chair: 34.01%\n",
      "predicted object chair: 20.55%\n",
      "predicted object chair: 78.90%\n",
      "predicted object chair: 80.24%\n",
      "predicted object chair: 52.60%\n",
      "predicted object backpack: 30.65%\n",
      "predicted object backpack: 31.35%\n",
      "predicted object chair: 29.54%\n",
      "predicted object chair: 30.36%\n",
      "predicted object person: 92.22%\n",
      "predicted object person: 92.20%\n",
      "predicted object person: 20.21%\n",
      "predicted object person: 23.58%\n",
      "predicted object backpack: 35.86%\n",
      "predicted object backpack: 37.84%\n",
      "predicted object backpack: 21.80%\n",
      "predicted object backpack: 26.22%\n",
      "predicted object backpack: 43.39%\n",
      "predicted object backpack: 44.30%\n",
      "predicted object backpack: 25.60%\n",
      "predicted object backpack: 28.65%\n",
      "predicted object chair: 34.22%\n",
      "predicted object chair: 84.94%\n",
      "predicted object chair: 88.32%\n",
      "predicted object chair: 35.36%\n",
      "predicted object chair: 45.16%\n",
      "predicted object backpack: 66.23%\n",
      "predicted object backpack: 65.88%\n",
      "predicted object backpack: 47.66%\n",
      "predicted object backpack: 48.03%\n",
      "predicted object chair: 46.12%\n",
      "predicted object chair: 46.61%\n",
      "predicted object chair: 27.37%\n",
      "predicted object chair: 89.49%\n",
      "predicted object chair: 90.30%\n",
      "predicted object chair: 66.05%\n",
      "predicted object backpack: 22.50%\n",
      "predicted object backpack: 23.52%\n",
      "predicted object chair: 45.26%\n",
      "predicted object chair: 45.93%\n",
      "predicted object person: 24.96%\n",
      "predicted object person: 81.57%\n",
      "predicted object person: 82.17%\n",
      "predicted object person: 72.14%\n",
      "predicted object person: 74.57%\n",
      "predicted object person: 26.17%\n",
      "predicted object person: 25.70%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted object backpack: 52.69%\n",
      "predicted object backpack: 54.88%\n",
      "predicted object backpack: 21.61%\n",
      "predicted object backpack: 22.61%\n",
      "predicted object backpack: 22.99%\n",
      "predicted object backpack: 51.52%\n",
      "predicted object backpack: 61.58%\n",
      "predicted object backpack: 25.51%\n",
      "predicted object backpack: 34.25%\n",
      "predicted object backpack: 40.74%\n",
      "predicted object backpack: 50.17%\n",
      "predicted object backpack: 57.59%\n",
      "predicted object backpack: 57.11%\n",
      "predicted object backpack: 77.00%\n",
      "predicted object backpack: 77.87%\n",
      "predicted object backpack: 75.01%\n",
      "predicted object backpack: 70.37%\n",
      "predicted object backpack: 70.47%\n",
      "predicted object backpack: 65.96%\n",
      "predicted object bed: 25.35%\n",
      "predicted object bed: 23.83%\n",
      "predicted object bed: 23.81%\n",
      "predicted object chair: 50.94%\n",
      "predicted object chair: 55.71%\n",
      "predicted object chair: 31.74%\n",
      "predicted object chair: 35.46%\n",
      "predicted object person: 60.77%\n",
      "predicted object person: 58.32%\n",
      "predicted object person: 28.82%\n",
      "predicted object person: 26.45%\n",
      "predicted object person: 52.84%\n",
      "predicted object person: 51.36%\n",
      "predicted object person: 24.87%\n",
      "predicted object person: 25.26%\n",
      "predicted object backpack: 57.30%\n",
      "predicted object chair: 60.60%\n",
      "predicted object chair: 57.13%\n",
      "predicted object chair: 58.60%\n",
      "predicted object chair: 55.63%\n",
      "predicted object chair: 20.68%\n",
      "predicted object bed: 22.45%\n",
      "predicted object bed: 34.53%\n",
      "predicted object backpack: 77.37%\n",
      "predicted object backpack: 78.70%\n",
      "predicted object backpack: 43.01%\n",
      "predicted object backpack: 44.29%\n",
      "predicted object backpack: 27.88%\n",
      "predicted object backpack: 54.81%\n",
      "predicted object backpack: 64.86%\n",
      "predicted object backpack: 23.25%\n",
      "predicted object backpack: 31.19%\n",
      "predicted object backpack: 35.51%\n",
      "predicted object backpack: 44.49%\n",
      "predicted object chair: 34.09%\n",
      "predicted object chair: 34.66%\n",
      "predicted object person: 29.15%\n",
      "predicted object person: 76.09%\n",
      "predicted object person: 49.72%\n",
      "predicted object backpack: 81.29%\n",
      "predicted object backpack: 80.19%\n",
      "predicted object backpack: 78.53%\n",
      "predicted object backpack: 79.35%\n",
      "predicted object backpack: 77.06%\n",
      "predicted object backpack: 67.80%\n",
      "predicted object backpack: 67.79%\n",
      "predicted object backpack: 63.47%\n",
      "predicted object chair: 36.16%\n",
      "predicted object chair: 35.80%\n",
      "predicted object bed: 26.15%\n",
      "predicted object bed: 24.99%\n",
      "predicted object bed: 24.39%\n",
      "predicted object chair: 29.17%\n",
      "predicted object chair: 22.42%\n",
      "predicted object chair: 22.99%\n",
      "predicted object chair: 25.47%\n",
      "predicted object chair: 28.58%\n",
      "predicted object chair: 27.15%\n",
      "predicted object chair: 30.66%\n",
      "predicted object person: 89.98%\n",
      "predicted object person: 91.21%\n",
      "predicted object person: 31.56%\n",
      "predicted object person: 41.59%\n",
      "predicted object person: 61.20%\n",
      "predicted object person: 70.35%\n",
      "predicted object backpack: 63.03%\n",
      "predicted object chair: 26.62%\n",
      "predicted object chair: 23.29%\n",
      "predicted object chair: 25.52%\n",
      "predicted object chair: 42.97%\n",
      "predicted object chair: 40.06%\n",
      "predicted object chair: 41.51%\n",
      "predicted object bed: 30.17%\n",
      "predicted object backpack: 82.68%\n",
      "predicted object backpack: 83.95%\n",
      "predicted object backpack: 52.26%\n",
      "predicted object backpack: 53.69%\n",
      "predicted object backpack: 27.35%\n",
      "predicted object backpack: 51.53%\n",
      "predicted object backpack: 62.07%\n",
      "predicted object backpack: 24.67%\n",
      "predicted object backpack: 33.17%\n",
      "predicted object backpack: 36.10%\n",
      "predicted object backpack: 45.26%\n",
      "predicted object chair: 28.13%\n",
      "predicted object chair: 29.42%\n",
      "predicted object chair: 21.52%\n",
      "predicted object person: 28.23%\n",
      "predicted object person: 70.09%\n",
      "predicted object person: 38.13%\n",
      "predicted object person: 35.73%\n",
      "predicted object backpack: 86.41%\n",
      "predicted object backpack: 85.50%\n",
      "predicted object backpack: 77.42%\n",
      "predicted object backpack: 78.30%\n",
      "predicted object backpack: 76.01%\n",
      "predicted object backpack: 65.03%\n",
      "predicted object backpack: 65.15%\n",
      "predicted object backpack: 60.62%\n",
      "predicted object chair: 59.83%\n",
      "predicted object chair: 59.43%\n",
      "predicted object chair: 52.67%\n",
      "predicted object chair: 29.94%\n",
      "predicted object chair: 39.74%\n",
      "predicted object chair: 21.96%\n",
      "predicted object chair: 26.16%\n",
      "predicted object chair: 25.92%\n",
      "predicted object chair: 24.54%\n",
      "predicted object chair: 28.27%\n",
      "predicted object person: 86.04%\n",
      "predicted object person: 87.11%\n",
      "predicted object person: 66.98%\n",
      "predicted object person: 74.65%\n",
      "predicted object person: 44.04%\n",
      "predicted object person: 56.01%\n",
      "predicted object person: 31.54%\n",
      "predicted object backpack: 61.62%\n",
      "predicted object chair: 35.03%\n",
      "predicted object chair: 31.09%\n",
      "predicted object chair: 51.54%\n",
      "predicted object chair: 49.05%\n",
      "predicted object chair: 36.13%\n",
      "predicted object bed: 33.26%\n",
      "predicted object backpack: 78.96%\n",
      "predicted object backpack: 80.38%\n",
      "predicted object backpack: 41.46%\n",
      "predicted object backpack: 42.54%\n",
      "predicted object backpack: 24.53%\n",
      "predicted object backpack: 46.99%\n",
      "predicted object backpack: 57.36%\n",
      "predicted object backpack: 25.01%\n",
      "predicted object backpack: 33.11%\n",
      "predicted object backpack: 36.40%\n",
      "predicted object backpack: 44.99%\n",
      "predicted object person: 54.99%\n",
      "predicted object person: 83.76%\n",
      "predicted object person: 61.14%\n",
      "predicted object backpack: 83.52%\n",
      "predicted object backpack: 82.60%\n",
      "predicted object backpack: 73.24%\n",
      "predicted object backpack: 74.17%\n",
      "predicted object backpack: 71.88%\n",
      "predicted object backpack: 60.25%\n",
      "predicted object backpack: 60.22%\n",
      "predicted object backpack: 55.28%\n",
      "predicted object bed: 30.53%\n",
      "predicted object bed: 29.38%\n",
      "predicted object bed: 28.62%\n",
      "predicted object chair: 24.10%\n",
      "predicted object chair: 33.63%\n",
      "predicted object chair: 22.03%\n",
      "predicted object chair: 22.44%\n",
      "predicted object chair: 24.51%\n",
      "predicted object chair: 31.36%\n",
      "predicted object chair: 29.87%\n",
      "predicted object chair: 33.39%\n",
      "predicted object person: 91.93%\n",
      "predicted object person: 93.40%\n",
      "predicted object person: 83.74%\n",
      "predicted object person: 89.67%\n",
      "predicted object backpack: 57.95%\n",
      "predicted object chair: 31.67%\n",
      "predicted object chair: 27.92%\n",
      "predicted object chair: 21.92%\n",
      "predicted object chair: 44.83%\n",
      "predicted object chair: 42.32%\n",
      "predicted object chair: 42.15%\n",
      "predicted object bed: 21.13%\n",
      "predicted object bed: 20.37%\n",
      "predicted object bed: 29.06%\n",
      "predicted object backpack: 79.62%\n",
      "predicted object backpack: 81.08%\n",
      "predicted object backpack: 49.28%\n",
      "predicted object backpack: 50.60%\n",
      "predicted object backpack: 26.87%\n",
      "predicted object backpack: 51.81%\n",
      "predicted object backpack: 62.01%\n",
      "predicted object backpack: 24.80%\n",
      "predicted object backpack: 33.34%\n",
      "predicted object backpack: 36.89%\n",
      "predicted object backpack: 45.93%\n",
      "predicted object person: 56.35%\n",
      "predicted object person: 87.91%\n",
      "predicted object person: 56.70%\n",
      "predicted object backpack: 83.03%\n",
      "predicted object backpack: 82.24%\n",
      "predicted object backpack: 77.10%\n",
      "predicted object backpack: 77.82%\n",
      "predicted object backpack: 75.42%\n",
      "predicted object backpack: 65.65%\n",
      "predicted object backpack: 65.76%\n",
      "predicted object backpack: 61.89%\n",
      "predicted object chair: 35.79%\n",
      "predicted object chair: 45.67%\n",
      "predicted object chair: 20.94%\n",
      "predicted object chair: 21.43%\n",
      "predicted object chair: 23.80%\n",
      "predicted object chair: 23.70%\n",
      "predicted object chair: 30.89%\n",
      "predicted object chair: 25.85%\n",
      "predicted object chair: 24.55%\n",
      "predicted object chair: 27.96%\n",
      "predicted object person: 93.27%\n",
      "predicted object person: 94.85%\n",
      "predicted object person: 83.97%\n",
      "predicted object person: 90.06%\n",
      "predicted object backpack: 61.71%\n",
      "predicted object chair: 40.97%\n",
      "predicted object chair: 37.03%\n",
      "predicted object chair: 55.46%\n",
      "predicted object chair: 52.59%\n",
      "predicted object chair: 33.66%\n",
      "predicted object bed: 27.85%\n",
      "predicted object backpack: 80.05%\n",
      "predicted object backpack: 81.37%\n",
      "predicted object backpack: 51.85%\n",
      "predicted object backpack: 53.38%\n",
      "predicted object backpack: 27.28%\n",
      "predicted object backpack: 53.10%\n",
      "predicted object backpack: 63.63%\n",
      "predicted object backpack: 22.40%\n",
      "predicted object backpack: 30.42%\n",
      "predicted object backpack: 35.59%\n",
      "predicted object backpack: 44.67%\n",
      "predicted object person: 57.52%\n",
      "predicted object person: 86.48%\n",
      "predicted object person: 52.92%\n",
      "predicted object backpack: 83.80%\n",
      "predicted object backpack: 82.89%\n",
      "predicted object backpack: 77.16%\n",
      "predicted object backpack: 77.93%\n",
      "predicted object backpack: 75.81%\n",
      "predicted object backpack: 65.05%\n",
      "predicted object backpack: 65.01%\n",
      "predicted object backpack: 61.24%\n",
      "predicted object chair: 26.19%\n",
      "predicted object chair: 33.58%\n",
      "predicted object chair: 29.44%\n",
      "predicted object chair: 30.10%\n",
      "predicted object chair: 32.58%\n",
      "predicted object chair: 35.91%\n",
      "predicted object chair: 34.40%\n",
      "predicted object chair: 37.69%\n",
      "predicted object person: 91.06%\n",
      "predicted object person: 93.02%\n",
      "predicted object person: 82.60%\n",
      "predicted object person: 88.76%\n",
      "predicted object backpack: 62.98%\n",
      "predicted object chair: 26.73%\n",
      "predicted object chair: 23.25%\n",
      "predicted object chair: 29.80%\n",
      "predicted object chair: 40.23%\n",
      "predicted object chair: 37.58%\n",
      "predicted object chair: 47.85%\n",
      "predicted object bed: 28.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted object backpack: 76.80%\n",
      "predicted object backpack: 78.32%\n",
      "predicted object backpack: 46.98%\n",
      "predicted object backpack: 48.17%\n",
      "predicted object backpack: 27.95%\n",
      "predicted object backpack: 52.20%\n",
      "predicted object backpack: 62.54%\n",
      "predicted object backpack: 25.60%\n",
      "predicted object backpack: 34.20%\n",
      "predicted object backpack: 37.72%\n",
      "predicted object backpack: 46.69%\n",
      "predicted object person: 64.66%\n",
      "predicted object person: 88.37%\n",
      "predicted object person: 47.19%\n",
      "predicted object backpack: 81.39%\n",
      "predicted object backpack: 80.47%\n",
      "predicted object backpack: 76.54%\n",
      "predicted object backpack: 77.41%\n",
      "predicted object backpack: 75.35%\n",
      "predicted object backpack: 65.67%\n",
      "predicted object backpack: 65.70%\n",
      "predicted object backpack: 61.07%\n",
      "predicted object bed: 22.76%\n",
      "predicted object bed: 21.55%\n",
      "predicted object bed: 20.82%\n",
      "predicted object chair: 24.70%\n",
      "predicted object chair: 34.25%\n",
      "predicted object chair: 25.94%\n",
      "predicted object chair: 26.46%\n",
      "predicted object chair: 28.80%\n",
      "predicted object chair: 21.62%\n",
      "predicted object chair: 33.80%\n",
      "predicted object chair: 32.18%\n",
      "predicted object chair: 35.88%\n",
      "predicted object person: 93.94%\n",
      "predicted object person: 95.23%\n",
      "predicted object person: 74.94%\n",
      "predicted object person: 87.35%\n",
      "predicted object backpack: 61.33%\n",
      "predicted object chair: 28.64%\n",
      "predicted object chair: 25.10%\n",
      "predicted object chair: 25.12%\n",
      "predicted object chair: 43.27%\n",
      "predicted object chair: 40.55%\n",
      "predicted object chair: 45.52%\n",
      "predicted object bed: 30.55%\n",
      "predicted object backpack: 77.41%\n",
      "predicted object backpack: 78.95%\n",
      "predicted object backpack: 45.70%\n",
      "predicted object backpack: 47.08%\n",
      "predicted object backpack: 28.35%\n",
      "predicted object backpack: 53.42%\n",
      "predicted object backpack: 63.65%\n",
      "predicted object backpack: 28.33%\n",
      "predicted object backpack: 37.58%\n",
      "predicted object backpack: 39.72%\n",
      "predicted object backpack: 48.66%\n",
      "predicted object person: 53.83%\n",
      "predicted object person: 86.83%\n",
      "predicted object person: 60.40%\n",
      "predicted object backpack: 83.67%\n",
      "predicted object backpack: 82.72%\n",
      "predicted object backpack: 76.08%\n",
      "predicted object backpack: 77.02%\n",
      "predicted object backpack: 74.73%\n",
      "predicted object backpack: 67.36%\n",
      "predicted object backpack: 67.42%\n",
      "predicted object backpack: 62.48%\n",
      "predicted object bed: 22.83%\n",
      "predicted object bed: 21.64%\n",
      "predicted object bed: 20.89%\n",
      "predicted object chair: 23.68%\n",
      "predicted object chair: 32.93%\n",
      "predicted object chair: 30.23%\n",
      "predicted object chair: 30.84%\n",
      "predicted object chair: 33.43%\n",
      "predicted object chair: 32.99%\n",
      "predicted object chair: 31.42%\n",
      "predicted object chair: 35.02%\n",
      "predicted object person: 91.88%\n",
      "predicted object person: 93.55%\n",
      "predicted object person: 84.07%\n",
      "predicted object person: 90.08%\n",
      "predicted object backpack: 59.70%\n",
      "predicted object chair: 27.40%\n",
      "predicted object chair: 23.98%\n",
      "predicted object chair: 32.60%\n",
      "predicted object chair: 37.45%\n",
      "predicted object chair: 35.26%\n",
      "predicted object chair: 45.84%\n",
      "predicted object bed: 33.52%\n",
      "predicted object backpack: 77.53%\n",
      "predicted object backpack: 78.86%\n",
      "predicted object backpack: 43.58%\n",
      "predicted object backpack: 45.14%\n",
      "predicted object backpack: 24.04%\n",
      "predicted object backpack: 50.92%\n",
      "predicted object backpack: 61.13%\n",
      "predicted object backpack: 25.49%\n",
      "predicted object backpack: 33.99%\n",
      "predicted object backpack: 38.42%\n",
      "predicted object backpack: 47.25%\n",
      "predicted object person: 64.35%\n",
      "predicted object person: 87.91%\n",
      "predicted object person: 52.16%\n",
      "predicted object backpack: 82.67%\n",
      "predicted object backpack: 81.77%\n",
      "predicted object backpack: 74.14%\n",
      "predicted object backpack: 75.04%\n",
      "predicted object backpack: 72.35%\n",
      "predicted object backpack: 63.81%\n",
      "predicted object backpack: 63.76%\n",
      "predicted object backpack: 58.67%\n",
      "predicted object chair: 31.66%\n",
      "predicted object chair: 40.85%\n",
      "predicted object chair: 20.36%\n",
      "predicted object chair: 20.70%\n",
      "predicted object chair: 22.66%\n",
      "predicted object chair: 20.23%\n",
      "predicted object chair: 27.40%\n",
      "predicted object chair: 27.76%\n",
      "predicted object chair: 26.41%\n",
      "predicted object chair: 29.79%\n",
      "predicted object person: 93.92%\n",
      "predicted object person: 95.27%\n",
      "predicted object person: 75.63%\n",
      "predicted object person: 87.24%\n",
      "predicted object backpack: 57.40%\n",
      "predicted object chair: 36.00%\n",
      "predicted object chair: 32.03%\n",
      "predicted object chair: 52.94%\n",
      "predicted object chair: 49.94%\n",
      "predicted object chair: 36.79%\n",
      "predicted object bed: 31.02%\n"
     ]
    }
   ],
   "source": [
    "# Open the laptop camera\n",
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "     # Preprocess the frame to create a blob object\n",
    "    scalefactor = 1.0/255.0\n",
    "    new_size = (416, 416)\n",
    "    blob = cv2.dnn.blobFromImage(frame, scalefactor, new_size, swapRB=True, crop=False)\n",
    "\n",
    "    # Input the preprocessed blob into the model\n",
    "    yolo_model.setInput(blob)\n",
    "    obj_detections_in_layers = yolo_model.forward(output_layers)\n",
    "\n",
    "    # Object detection analysis and visualization\n",
    "    confidence_threshold = 0.2\n",
    "    result_raw = object_detection_analysis(frame, obj_detections_in_layers, confidence_threshold)\n",
    "    \n",
    "    # Display the resulting frame with bounding boxes and class labels\n",
    "    cv2.imshow('Object Detection',result_raw)\n",
    "\n",
    "    # Press 'q' key to exit the loop and stop the camera stream\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f343578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids_list = []\n",
    "boxes_list = []\n",
    "confidences_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "854f6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection_attributes(frame, obj_detections_in_layers, confidence_threshold):\n",
    "  # get the image dimensions  \n",
    "  img_height = frame.shape[0]\n",
    "  img_width = frame.shape[1]\n",
    "  \n",
    "  # loop over each output layer \n",
    "  for object_detections_in_single_layer in obj_detections_in_layers:\n",
    "    # loop over the detections in each layer\n",
    "    for object_detection in object_detections_in_single_layer:  \n",
    "      # get the confidence scores of all objects detected with the bounding box\n",
    "      prediction_scores = object_detection[5:]\n",
    "      # consider the highest score being associated with the winning class\n",
    "      # get the class ID from the index of the highest score \n",
    "      predicted_class_id = np.argmax(prediction_scores)\n",
    "      # get the prediction confidence\n",
    "      prediction_confidence = prediction_scores[predicted_class_id]\n",
    "      \n",
    "      # consider object detections with confidence score higher than threshold\n",
    "      if prediction_confidence > confidence_threshold:\n",
    "        # get the predicted label\n",
    "        predicted_class_label = class_labels[predicted_class_id]\n",
    "        # compute the bounding box coordinates scaled for the input image\n",
    "        bounding_box = object_detection[0:4] * np.array([img_width, img_height, img_width, img_height])\n",
    "        (box_center_x_pt, box_center_y_pt, box_width, box_height) = bounding_box.astype(\"int\")\n",
    "        start_x_pt = max(0, int(box_center_x_pt - (box_width / 2)))\n",
    "        start_y_pt = max(0, int(box_center_y_pt - (box_height / 2)))\n",
    "        \n",
    "      \n",
    "        class_ids_list.append(predicted_class_id)\n",
    "        confidences_list.append(float(prediction_confidence))\n",
    "        boxes_list.append([int(start_x_pt), int(start_y_pt), int(box_width), int(box_height)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "775e92bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted object person: 95.69%\n",
      "predicted object backpack: 84.47%\n",
      "predicted object backpack: 76.32%\n",
      "predicted object chair: 52.86%\n",
      "predicted object person: 94.77%\n",
      "predicted object backpack: 86.49%\n",
      "predicted object backpack: 78.03%\n",
      "predicted object person: 95.45%\n",
      "predicted object backpack: 82.55%\n",
      "predicted object backpack: 78.37%\n",
      "predicted object person: 96.01%\n",
      "predicted object backpack: 82.24%\n",
      "predicted object backpack: 75.16%\n",
      "predicted object chair: 55.47%\n",
      "predicted object person: 95.77%\n",
      "predicted object backpack: 84.60%\n",
      "predicted object backpack: 75.26%\n",
      "predicted object person: 95.98%\n",
      "predicted object backpack: 83.60%\n",
      "predicted object backpack: 76.97%\n",
      "predicted object person: 96.31%\n",
      "predicted object backpack: 85.35%\n",
      "predicted object backpack: 76.58%\n",
      "predicted object chair: 53.67%\n",
      "predicted object person: 95.72%\n",
      "predicted object backpack: 84.20%\n",
      "predicted object backpack: 77.81%\n",
      "predicted object chair: 52.43%\n",
      "predicted object person: 94.51%\n",
      "predicted object backpack: 83.07%\n",
      "predicted object backpack: 80.50%\n",
      "predicted object chair: 54.31%\n",
      "predicted object person: 95.34%\n",
      "predicted object backpack: 79.77%\n",
      "predicted object backpack: 79.30%\n",
      "predicted object chair: 50.01%\n",
      "predicted object person: 94.81%\n",
      "predicted object backpack: 83.22%\n",
      "predicted object backpack: 78.28%\n",
      "predicted object person: 94.97%\n",
      "predicted object backpack: 80.53%\n",
      "predicted object backpack: 77.11%\n",
      "predicted object person: 95.16%\n",
      "predicted object backpack: 82.18%\n",
      "predicted object backpack: 77.47%\n",
      "predicted object chair: 51.59%\n",
      "predicted object person: 95.10%\n",
      "predicted object backpack: 84.45%\n",
      "predicted object backpack: 75.46%\n",
      "predicted object chair: 53.81%\n",
      "predicted object person: 93.83%\n",
      "predicted object backpack: 86.52%\n",
      "predicted object backpack: 76.49%\n",
      "predicted object person: 94.65%\n",
      "predicted object backpack: 82.01%\n",
      "predicted object backpack: 77.52%\n",
      "predicted object chair: 51.74%\n",
      "predicted object person: 94.24%\n",
      "predicted object backpack: 83.20%\n",
      "predicted object backpack: 79.10%\n",
      "predicted object chair: 54.75%\n",
      "predicted object person: 94.69%\n",
      "predicted object backpack: 82.92%\n",
      "predicted object backpack: 77.29%\n",
      "predicted object chair: 50.14%\n",
      "predicted object person: 94.21%\n",
      "predicted object backpack: 87.17%\n",
      "predicted object backpack: 76.16%\n",
      "predicted object chair: 59.28%\n",
      "predicted object person: 96.47%\n",
      "predicted object backpack: 82.12%\n",
      "predicted object backpack: 75.21%\n",
      "predicted object backpack: 79.35%\n",
      "predicted object backpack: 72.13%\n",
      "predicted object chair: 50.19%\n",
      "predicted object backpack: 91.67%\n",
      "predicted object backpack: 83.33%\n",
      "predicted object chair: 72.94%\n",
      "predicted object backpack: 93.22%\n",
      "predicted object backpack: 83.74%\n",
      "predicted object chair: 79.60%\n",
      "predicted object backpack: 89.33%\n",
      "predicted object backpack: 83.64%\n",
      "predicted object chair: 79.52%\n",
      "predicted object backpack: 91.79%\n",
      "predicted object backpack: 83.55%\n",
      "predicted object chair: 83.51%\n",
      "predicted object backpack: 91.76%\n",
      "predicted object backpack: 83.89%\n",
      "predicted object chair: 68.52%\n",
      "predicted object backpack: 88.78%\n",
      "predicted object backpack: 83.77%\n",
      "predicted object chair: 81.86%\n",
      "predicted object backpack: 90.65%\n",
      "predicted object backpack: 86.83%\n",
      "predicted object chair: 80.32%\n",
      "predicted object backpack: 92.47%\n",
      "predicted object backpack: 83.23%\n",
      "predicted object chair: 75.36%\n",
      "predicted object backpack: 90.32%\n",
      "predicted object backpack: 84.18%\n",
      "predicted object chair: 62.70%\n"
     ]
    }
   ],
   "source": [
    "# Open the laptop camera\n",
    "cap = cv2.VideoCapture(0) \n",
    "while True:\n",
    "\n",
    "    # Read a frame from the camera\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    #frame = cv2.flip(frame,1)\n",
    "    # Check if the frame was read successfully\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    #Preprocess the frame to create a blob object\n",
    "    scalefactor = 1.0/255.0\n",
    "    new_size = (416, 416)\n",
    "    blob = cv2.dnn.blobFromImage(frame, scalefactor, new_size, swapRB=True, crop=False)\n",
    "    \n",
    "    #Input the preprocessed blob into the model\n",
    "    yolo_model.setInput(blob)\n",
    "    obj_detections_in_layers = yolo_model.forward(output_layers)\n",
    "    score_threshold = 0.5\n",
    "    object_detection_attributes(frame, obj_detections_in_layers, score_threshold)\n",
    "    \n",
    "    score_threshold = 0.5\n",
    "    nms_threshold = 0.4\n",
    "    winner_ids = cv2.dnn.NMSBoxes(boxes_list, confidences_list, score_threshold, nms_threshold)\n",
    "    \n",
    "    # loop through the final set of detections\n",
    "    for winner_id in winner_ids:\n",
    "        max_class_id = winner_id\n",
    "        box = boxes_list[max_class_id]\n",
    "        start_x_pt = box[0]\n",
    "        start_y_pt = box[1]\n",
    "        box_width = box[2]\n",
    "        box_height = box[3]\n",
    "    \n",
    "        #get the predicted class id and label\n",
    "        predicted_class_id = class_ids_list[max_class_id]\n",
    "        predicted_class_label = class_labels[predicted_class_id]\n",
    "        prediction_confidence = confidences_list[max_class_id]\n",
    " \n",
    "        #obtain the bounding box end coordinates\n",
    "        end_x_pt = start_x_pt + box_width\n",
    "        end_y_pt = start_y_pt + box_height\n",
    "    \n",
    "        #get a random mask color from the numpy array of colors\n",
    "        box_color = class_colors[predicted_class_id]\n",
    "    \n",
    "        #convert the color numpy array as a list and apply to text and box\n",
    "        box_color = [int(c) for c in box_color]\n",
    "    \n",
    "        # print the prediction in console\n",
    "        predicted_class_label = \"{}: {:.2f}%\".format(predicted_class_label, prediction_confidence * 100)\n",
    "        print(\"predicted object {}\".format(predicted_class_label))\n",
    "    \n",
    "        # draw rectangle and text in the image\n",
    "        cv2.rectangle(frame, (start_x_pt, start_y_pt), (end_x_pt, end_y_pt), box_color, 2)\n",
    "        cv2.putText(frame, predicted_class_label, (start_x_pt, start_y_pt-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 2)\n",
    "    #cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    \n",
    "    # Display the resulting frame with bounding boxes and class labels\n",
    "    cv2.imshow('Object Detection',frame)\n",
    "    class_ids_list = []\n",
    "    boxes_list = []\n",
    "    confidences_list = []\n",
    "\n",
    "    # Press 'q' key to exit the loop and stop the camera stream\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775fdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
